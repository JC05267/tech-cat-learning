# 06/17/2024
# Data Engineering Bootcamp - DAY 1

- Couple of breaks
- Lunch break at 12, except on Friday; 12:45 or 1
- 8-3: Tarek, 3-close: Harshit
    - Taret in Jordan, Harshit in India (timezones)
- Goal: Appreciate the value of data engineering.
- 8 weeks of learning
- book: Deciphering Data Architecture

## Weekly Breakdown (8 weeks of instruction)
1. Data Engineering on the Cloud - especially on AWS
2. Python for Data Engineering and Analytics
3. SQL for Data Engineering and Analytics
4. Advanced Snowflake
5. Data Engineering pipelines - linux, git, terminal
6. DevOps, DataOps, MLOps, LLMOps
7. Data Visualization - Tableau and ThoughtSpot
8. Capstone Project - End-to-End project to demonstrate what you learned (finalized by week 5 or so)

All content is on github.

Fundamentals -> AWS tools -> Python -> SQL -> Snowflake -> Visualization

Use `gartner magic quadrant` to get an idea of industry leaders on a technology.

# Data Engineering on the Cloud 

## What is Data Engineering
Designing and building systems to collect, process and manage large amounts of data from various sources and formats.
Data Pipelines, Data warehouses, Data lakes enable analysis and use of data for business insights and decision-making.

Data should be reliable, consistent, and easily accessible for data scientists and other stakeholders.

## Common tasks
- acquire datasets that align with business needs
- Develop algorithms to transform data into useful information
- Build, test and maintain database pipeline architectures
- Collaborate with management understand company objectives
- New data validation methods and analysis tools
- data governance and security policies

## Skills
- Warehousing and ETL
- Programming
- Hadoop 
- SQL + Database
- Data Architecture + Pipeline
- Machine Learning 
- Scripting, Reporting, Data Visualization

## Tech Stack - things that matter when considering tech
- license
- coding level
- scaling 
- deployment - on premise vs cloud
- user accessibility

## End Goal
- Data driven decision making
- guide strategic business decisions

## Types of Data Analytics
1. Descriptive - What is happening to my business?
    - pie charts, bar graphs, etc
    - dashboards
2. Diagnostic - Why is it happening?
    - Causes of events and behaviors
    - drill down charts, scatter plats, waterfall charts
3. Predictive - What is likely to happen?
    - stats and machine learning to determine likelihood of future outcomes 
    - forecast plots, probability density plots, regression plots
4. Prescriptive - What do I need to do?
    - recommendations to affect desired outcomes 
    - decision trees, recommendation engine outputs, simulation models

Maturity models are useful for understanding where the company is in its process.

## What is Big Data
- tons of data, can't be processed with traditional data techniques
- Volume, Variety, Velocity (three V's) 
### Six V's
- Volume - lots of data 
- Velocity - real-time, batch 
- Value - projections, trends etc
- Variability - lots of use cases 
- Veracity - varying degrees of trust 
- Variety - apps, databases, social media etc

## Diversity in Data Sources
- on-prem
- cloud
- Databases
    - Relational
    - Non-Relational
    - File-based (excel, parquet, etc)
- forms
    - structured - excel
    - unstructured (unstructured) - images, emails
    - semi-structured (json)
- speed
    - intervals - daily, weekly, monthly
    - real time

## Structured Data
- defined schema
- e.g. database tables, csv, excel
- easily queryable
- organized to rows and columns 
- consistent structure
- types, format are well defined

## Unstructured Data
- no predefined structure or schema
- schema typically set at runtime
- tends to be text heavy
- Not easily queryable without preprocessing
- various formats
- e.g. images, video and audio, emails

## Semi-structured
- not as organized, but still has some structure
- XML, json, etc

## Sources
- External - often at a cost
- Internal

## Making Data Useable
- Source of truth
- Data Warehouse keeps load off sources
- 1980's = Data Warehouse -> 2011 Data Lake (popularized by Databricks) -> 2020 Lakehouse

## Data Warehouse
- ETL
- transformed data into a defined schema
- complex queries and analysis
- can't handle unstructured data
- traditional data warehouses can cost more then data lakes due to additional operation costs
- e.g. Amazon Redshift, Google BigQuery, Azure Synapse Analytics, Snowflake
- need to do ETL before put in
- schema-on-write
- star schema

## Data Lake
- ELT
- just storage, so much more affordable
- hold raw data
- steaming and appending data is difficult
- e.g. hadoop, AWS s3 
- schema-on-read
- hard for non-technical users to take advantage of
- Folders for each layer: Raw -> Conformed (parquet) -> Enriched -> Curated

## Data Lakehouse (Delta Lake)
- a bit of warehouse and a bit of lake
- ETL
- Singe data repository
- lower data storage cost
- Time consuming to build (relatively new so lacking tooling as of 2024)
- Data lake with additional functionality
- non-technical people might confuse lake house with data lake
- capable of tracking new data
 
## Data Mart
- focused repository of data that is optimized to meet the specific needs of a department or line of business within an organization.
- e.g. HR, Marketing, Finance each get their own mart

## Evolution Summary
- Data warehouse -> Data Lake -> Data Fabric -> Data Lakehouse

## File Formats
### CSV (Comma Separated Value)
- Text-based format with commas
- small - medium datasets
- human readable
- easy import/export from databases
### XML (Extensible Markup Language)
- tag system similar to html
- redundancy in XML because of tags
- human readable (not as readable as csv)
### JSON (Java Script Object Notation)
- text based
- human readable
- data interchange
- flexible schema and nested data
- often in web server/client
- redundancy
### AVRO
- binary format
- stores data and schema - separates the two
- big data and real time processing
- often used in Kafka
### Parquet
- columnar storage
- good compression
- analyzing large datasets (analytics workloads)
- use where i/o operations and storage need optimization
- separate schema file
### ORC (Optimized Row Columnar)
- columnar
- separate schema file
- high performance read/write
- often used in hive
