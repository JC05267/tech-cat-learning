# 06/18/2024
# Data Engineering Bootcamp - Day #2

## Big Data Stack
- Analytics
- Low Cost Computing
- High Volume Low Cost Storage

The Cloud separates storage and compute.

- DW = Data Warehouse 
- RDW = Relational Data Warehouse
- ETL = Extract, Transform, Load

## Data Sources
- JDBC - data database connectivity
- ODBC - open database connectivity
- Raw Log Files
- API's
- Streams
- CSV
- XML
- JSON
- Avro
- Parquet
- ORC

## Data Engineering Process
|----------- Orchestration ----------|
Ingestion -> Transformation -> Serving

## ETL Pipelines
- ETL = extract, transform, load
- used to move data from source systems into a data warehouse
### Extract
- retrieve raw data
- from database, crm, flat files, apis, etc
- ensure data integrity during extraction phase
- real time, or in batches
### Transformation
- convert extracted data into format suitable for target data warehouse
- Operations:
    - Cleaning
    - Enrichment
    - formatting
    - aggregations/computations
    - encoding/decoding
    - missing values
### Load
- move transformed data into target data warehouse or data repository
- done in batch or in streaming manner
- ensure data maintains its integrity
### ETL, ELT, Reverse ETL
- ETL: Source -> extract, transform, load -> DW
- ELT: extract, load, transform (common in data lake)
- Reverse: DW -> extract, transform, load -> Source

## Approximating Project Timelines 
- Mostly experienced based, will come with time
- Ask about existing processes and scripts: how long did they take?
    - use this to gather data
    - time of completion of existing projects can give you an idea of how long projects take in the company

## Orchestration
- Process of dependency management facilitated through automation
- workflow management
- automation
- error handling
- monitoring, alerting
- recovery
- observability
- debugging
- compliance and auditing

## Types of Cloud Computing
- On-prem (everything managed by you)
- Infrastricture as a Service
- Platform as a Service
- Software as a Service (little managed by you)

### AWS Shared Responsibility Model
- Responsibility is managed by user and cloud
- cost and security implications

- AWS Redshift competes with Snowflake

## AWS S3 - Buckets
- Store "Objects" in buckets - think of them as directories
- mist be globally unique (across all regions and accounts)
- objects have a key
    - Key is the full path: s3://<my bucket>/<file name>
- there is not actually a concept of directories, it just seems that way
## Bucket Policy
- json based policies
    - allow or deny
- By default, blocked all public access
- You can version files in s3, not enabled by default
- same key overwrite will update version
- replication: 
    - across region
    - same region
    - copying is asynchronous
- Can transition objects between storage classes
- for infrequently accessed data, move to "deeper" class
- event notifications
    - trigger on various event, e.g. objected created, deleted etc


